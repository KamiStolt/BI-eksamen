{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI Exam May 2025: COVID-19 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created by Group 7 - Kamilla, Jeanette, Juvena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sm\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# Set plot styles for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tools ready, the next step is to load the COVID-19 dataset into Python so we can start analyzing it.\n",
    "\n",
    "In this case, we’re working with a single dataset:\n",
    "\n",
    "- **OWID COVID-19 Latest Data**: a CSV file that contains country-level information on cases, deaths, vaccinations, testing, and various socioeconomic indicators.\n",
    "\n",
    "We'll use Pandas to read the CSV file and store it as a DataFrame. To make our code cleaner and reusable, we'll define a simple function that loads the data and performs some initial checks. This way, we can easily reload or replace the dataset if needed in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the covid datasets. (dataset: last updated 2024-08-04)\n",
    "dataset_covid = 'Data/owid-covid-latest.csv'\n",
    "\n",
    "# Function to load the Excel files\n",
    "def load_csv_to_dataframe(file_path):\n",
    "    # Reads the Excel file and skips the first row if it contains a description or title\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Load datasets\n",
    "print(\"..Loading COVID-19 dataset\")\n",
    "df_covid = load_csv_to_dataframe(dataset_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we want to explore it to understand what kind of information it contains and how it's structured.\n",
    "\n",
    "To do this, we can use several helpful Pandas functions such as `shape`, `types`, `info()`, `head()`, `tail()`, `sample()`, `describe()` and `isnull().sum()`. These functions will give us insights into the number of rows and columns, the data types of each column, a summary of the data, and any missing values. \n",
    "\n",
    "This exploration is crucial as it helps us identify potential issues or areas that need further cleaning or transformation before we proceed with our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame (rows, columns)\n",
    "df_covid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the types of attributes (colum names) in the DataFrame\n",
    "df_covid.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives an overview of the DataFrame\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the DataFrame\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the DataFrame\n",
    "df_covid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 5 rows from the DataFrame\n",
    "df_covid.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives summary statistics for all numerical columns in the dataset\n",
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.1 Summary of exploring the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the dataframe, we found that it contains a large number of columns, many of which are not useful for our analysis or modeling goals. While some columns provide valuable information (like total cases, deaths, and vaccination rates), others are either redundant, mostly empty, or irrelevant.\n",
    "\n",
    "This highlights the need for a thorough data cleaning step to remove unnecessary columns, handle missing values, and focus only on the most relevant features for our machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading and exploring the data, we need to clean it to ensure that our analysis is accurate and meaningful. Data cleaning involves several steps, including: checking for missing values, removing duplicates, and converting data types.\n",
    "\n",
    "We start by doing a bit of cleaning of the big dataset, to remove rows and columns that are not relevant for our futher analysis and before we seperate the data into more specific datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_covid.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that many columns contain no values at all, so we will remove them to clean up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning the data, we want to remove irrelevant OWID aggregate rows—such as those representing high-income, low-income, and other income groupings.\n",
    "rows_to_remove = [\"OWID_UMC\", \"OWID_WRL\", \"OWID_LMC\", \"OWID_LIC\", \"OWID_HIC\"]\n",
    "df_removed_rows = df_covid[~df_covid[\"iso_code\"].isin(rows_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are removing the 'low-income countries', 'lower-middle-income countries', 'upper-middle-income countries', 'high-income countries' and 'world' categories because they are too broad and lack specific country-level detail, making it difficult to draw meaningful conclusions without relying on assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the above rows were removed\n",
    "print(f\"{df_covid.shape}\")\n",
    "print(f\"Removed the {df_covid.shape[0] - df_removed_rows.shape[0]} OWID rows from the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop all columns with no values at all like; excess_mortality_cumulative_absolute, excess_mortality_cumulative etc.\n",
    "df_covid_removed_columns = df_removed_rows.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the columns were removed\n",
    "print(f\"COVID dataframe shape after removing columns: {df_covid_removed_columns.shape}\")\n",
    "print(f\"Removed {df_covid.shape[1] - df_covid_removed_columns.shape[1]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Separating the data into different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate the continent-level , age-level and health-level data into their own DataFrames so that we can clean and process them independently from the country-level data. This allows us to apply different cleaning steps based on the nature of the data, since data may have different structures or missing values compared to individual countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 Separating the continent-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the DataFrame based on a list of values\n",
    "def filter_dataframe(df, values, filter_type='rows', row_filter_column=None):\n",
    "    if filter_type == 'rows':\n",
    "        if row_filter_column is None:\n",
    "            raise ValueError(\"Must specify 'row_filter_column' when filtering rows.\")\n",
    "        return df[df[row_filter_column].isin(values)]\n",
    "    elif filter_type == 'columns':\n",
    "        # Keep only columns present in df and in values list (avoid key error)\n",
    "        columns_to_keep = [col for col in values if col in df.columns]\n",
    "        return df[columns_to_keep]\n",
    "    else:\n",
    "        raise ValueError(\"filter_type must be either 'rows' or 'columns'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before removing the iso_code column, we want to secure the OWID fields for the continents since it could be relevant data to analyze.\n",
    "rows_to_secure = [\"OWID_AFR\", \"OWID_ASI\", \"OWID_EUR\", \"OWID_EUN\", \"OWID_NAM\", \"OWID_OCE\", \"OWID_SAM\"]\n",
    "df_continents = filter_dataframe(df_covid_removed_columns, rows_to_secure, filter_type='rows', row_filter_column='iso_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the rows were secured\n",
    "df_continents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new seperate dataframe called `df_continent` that contains the continent-level data. This DataFrame will be used for further analysis and modeling, while the original `df_covid` DataFrame will focus on country-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns that are irrelavnt since they contain no values at all like; population_density, median_age etc.\n",
    "df_continents_romved_columns = df_continents.dropna(axis=1, how='all')\n",
    "df_continents_romved_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns `new_vaccinations_smoothed_per_million`, `new_people_vaccinated_smoothed` and `new_people_vaccinated_smoothed_per_hundred` contain a lot of missing values so they are not necessary for our analysis and we will drop them from the `df_continents_cleaned` DataFrame. By removing them, we can simplify the DataFrame and focus on the most relevant features for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the columns were removed\n",
    "print(f\"df_continent shape after removing columns: {df_continents_romved_columns.shape}\")\n",
    "print(f\"Removed {df_continents.shape[1] - df_continents_romved_columns.shape[1]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns there are irrelevant \n",
    "df_continents_cleaned = df_continents_romved_columns.drop(['iso_code', 'new_vaccinations_smoothed_per_million', 'new_people_vaccinated_smoothed', 'new_people_vaccinated_smoothed_per_hundred'], axis=1)\n",
    "df_continents_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "df_continents_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some rows with missing values in the df_continents_cleaned DataFrame, so we will impute them to ensure that our analysis is accurate and meaningful. This step is important because missing values can lead to biased results or errors in our models.\n",
    "\n",
    "Since it is a small dataframe, we can't delete the row with missing values(NaN), since we will lose a lot of data. We choose to replace the missing values, even though it can have a high risk of giving wrong information and have a big impact. \n",
    "\n",
    "The first four we replaced using realistic data.\n",
    "But since it took too much time, we’ll use the median to fill the remaining NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for replacing cell with a value\n",
    "def replace_cell(df, row_filter, column, value):\n",
    "    df.loc[row_filter, column] = value\n",
    "\n",
    "# replace NaN for total_vaccinations for Africa. 1.084.500.000 is from Africa CDC, which is offical and reliable.\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'Africa', 'total_vaccinations', 1084500000)\n",
    "\n",
    "# replace NaN for total_vaccinations for South America. 970.800.000 is from WTO-IMF COVID-19 Vaccine Trade Tracker, which is offical and reliable.\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'South America', 'total_vaccinations', 970800000)\n",
    "\n",
    "# replace NaN for people_vaccinated for Africa. 725.000.000 is from Africa CDC, which is offical and reliable\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'Africa', 'people_vaccinated', 725000000)\n",
    "\n",
    "# replace NaN for people_vaccinated for South America. 351.310.000 is from Our World in Data which is offical and reliable used by WHO.\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'South America', 'people_vaccinated', 351310000)\n",
    "\n",
    "# method for replacing cell with median \n",
    "def fill_na_with_median(df, column_name):\n",
    "    median_value = df[column_name].median()\n",
    "    print(f\"Median of '{column_name}': {median_value:.2f}\")\n",
    "    df[column_name].fillna(median_value, inplace=True)\n",
    "\n",
    "\n",
    "fill_na_with_median(df_continents_cleaned, 'people_fully_vaccinated')\n",
    "fill_na_with_median(df_continents_cleaned, 'total_boosters')\n",
    "fill_na_with_median(df_continents_cleaned, 'new_vaccinations')\n",
    "fill_na_with_median(df_continents_cleaned, 'new_vaccinations_smoothed')\n",
    "fill_na_with_median(df_continents_cleaned, 'total_vaccinations_per_hundred')\n",
    "fill_na_with_median(df_continents_cleaned, 'people_vaccinated_per_hundred')\n",
    "fill_na_with_median(df_continents_cleaned, 'people_fully_vaccinated_per_hundred')\n",
    "fill_na_with_median(df_continents_cleaned, 'total_boosters_per_hundred')\n",
    "df_continents_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_continents_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Separating the age-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the same function as above to seperate the age-level columns from the rest of the data.\n",
    "columns_to_secure = [\"continent\", \"location\", \"total_deaths_per_million\", \"median_age\", \"aged_65_older\", \"aged_70_older\", \"life_expectancy\"]\n",
    "df_age = filter_dataframe(df_covid_removed_columns, columns_to_secure, filter_type='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the rows were secured\n",
    "df_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new seperate dataframe called `df_age` that contains the age-level data. This DataFrame will be used for further analysis and modeling, while the original `df_covid` DataFrame will focus on country-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_age.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the rows with NaN values in the 'median_age' column\n",
    "df_age_cleaned = df_age.dropna(subset=['median_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do another check for missing values in the DataFrame\n",
    "df_age_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some missing values in the `df_age_cleaned` DataFrame, so we will impute them to ensure that our analysis is accurate and meaningful. This step is important because missing values can lead to biased results or errors in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with the median for the columns; total_deaths_per_million, aged_65_older and aged_70_older\n",
    "fill_na_with_median(df_age_cleaned, \"total_deaths_per_million\")\n",
    "fill_na_with_median(df_age_cleaned, \"aged_65_older\")\n",
    "fill_na_with_median(df_age_cleaned, \"aged_70_older\")\n",
    "df_age_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "df_age_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Separating the health-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the same function as above to seperate the health-level columns from the rest of the data.\n",
    "columns_to_secure = [\"continent\", \"location\", \"total_deaths_per_million\", \"cardiovasc_death_rate\", \"diabetes_prevalence\", \"female_smokers\", \"male_smokers\", \"life_expectancy\"]\n",
    "df_health = filter_dataframe(df_covid_removed_columns, columns_to_secure, filter_type='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the rows were secured\n",
    "df_health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new seperate dataframe called `df_health` that contains age-level data. This DataFrame will be used for further analysis and modeling, while the original `df_covid` DataFrame will focus on country-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_health.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the rows with NaN values in the 'female_smokers' column\n",
    "df_health_cleaned = df_health.dropna(subset=['female_smokers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do another check for missing values in the DataFrame\n",
    "df_health_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some missing values in the `df_health_cleaned` DataFrame, so we will impute them to ensure that our analysis is accurate and meaningful. This step is important because missing values can lead to biased results or errors in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with the median for the columns; cardiovasc_death_rate and male_smokers\n",
    "fill_na_with_median(df_health_cleaned, \"cardiovasc_death_rate\")\n",
    "fill_na_with_median(df_health_cleaned, \"male_smokers\")\n",
    "df_health_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "df_health_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Futher cleaning of the country-level data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected a subset of columns that we consider relevant for our analysis. This subset includes columns that provide information on total cases, deaths and population. By focusing on these columns, we can simplify our analysis and make it easier to draw meaningful conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new dataframe with the columns we want to keep for future analysis.\n",
    "columns_we_want_to_keep = [\n",
    "    \"iso_code\", \"continent\", \"location\", \"total_cases\", \"total_deaths\",\n",
    "    \"total_cases_per_million\", \"total_deaths_per_million\",\n",
    "    \"life_expectancy\", \"population\"]\n",
    "\n",
    "# Removes all other columns\n",
    "df_covid = df_covid_removed_columns[columns_we_want_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the columns were removed\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load another dataset so we can add data about the Human Development Index (HDI) for each country. The HDI is a composite index of life expectancy, education, and per capita income indicators, which are used to rank countries into four tiers of human development. This additional information will help us better understand the relationship between COVID-19 and various socioeconomic factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the new dataset\n",
    "hdi = pd.read_csv('Data/human-development-index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can then add a column with the HDI data for 2021 matching the countries in the covid dataset, because we only need data from the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter HDI for 2021 only\n",
    "hdi_2021 = hdi[hdi['Year'] == 2021]\n",
    "\n",
    "# Merge using 'location' from df_covid and 'Entity' from hdi\n",
    "df_merged = df_covid.merge(\n",
    "    hdi_2021[['Entity', 'Human Development Index']], \n",
    "    left_on='location', \n",
    "    right_on='Entity', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'Entity' column after merge, since we don't need it\n",
    "df_merged = df_merged.drop(columns=['Entity'])\n",
    "\n",
    "# Rename the column in the merged dataframe\n",
    "df_merged = df_merged.rename(columns={'Human Development Index': 'human_development_index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the dataset look and how we should proceed\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataframe after some cleaning\n",
    "print(f\"COVID dataframe shape after removing both some columns and rows: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are isolating the remaining rows in the df_covid DataFrame to ensure it contains only country-level data. This allows us to clean the dataset and retain only the features that are most relevant for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we seperated the OWID continent fields into it's own dataframe earlier, we now have to remove them again for the df_covid dataframe.\n",
    "rows_to_remove = [\"OWID_AFR\", \"OWID_ASI\", \"OWID_EUR\", \"OWID_EUN\", \"OWID_NAM\", \"OWID_OCE\", \"OWID_SAM\"]\n",
    "df_covid_removed_rows = df_merged[~df_merged['iso_code'].isin(rows_to_remove)]\n",
    "df_covid_cleaned = df_covid_removed_rows.dropna(subset=['iso_code'])\n",
    "df_covid_cleaned = df_covid_cleaned.drop(columns=['iso_code'])\n",
    "df_covid_cleaned        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the rows were removed\n",
    "print(f\"COVID dataframe shape after removing rows: {df_covid_cleaned.shape}\")\n",
    "print(f\"Removed {df_merged.shape[0] - df_covid_cleaned.shape[0]} rows from the dataframe.\")\n",
    "print(f\"Removed {df_merged.shape[1] - df_covid_cleaned.shape[1]} column from the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_covid_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a lot of missing values for the human_development_index column, so we will impute them with the HDI from the sovereign countries they belong too. This step is important because missing values can lead to biased results or errors in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which locations have missing HDI values\n",
    "missing_hdi_locations = df_covid_cleaned[df_covid_cleaned['human_development_index'].isna()]\n",
    "print(missing_hdi_locations['location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "territory_to_country = {\n",
    "    'American Samoa': 'United States',\n",
    "    'Anguilla': 'United Kingdom',\n",
    "    'Aruba': 'Netherlands',\n",
    "    'Bermuda': 'United Kingdom',\n",
    "    'Bonaire Sint Eustatius and Saba': 'Netherlands',\n",
    "    'British Virgin Islands': 'United Kingdom',\n",
    "    'Cayman Islands': 'United Kingdom',\n",
    "    'Cook Islands': 'New Zealand',\n",
    "    'Curacao': 'Netherlands',\n",
    "    'Falkland Islands': 'United Kingdom',\n",
    "    'Faroe Islands': 'Denmark',\n",
    "    'French Guiana': 'France',\n",
    "    'French Polynesia': 'France',\n",
    "    'Gibraltar': 'United Kingdom',\n",
    "    'Greenland': 'Denmark',\n",
    "    'Guadeloupe': 'France',\n",
    "    'Guam': 'United States',\n",
    "    'Guernsey': 'United Kingdom',\n",
    "    'Isle of Man': 'United Kingdom',\n",
    "    'Jersey': 'United Kingdom',\n",
    "    'Kosovo': 'Serbia',  # or leave as is if Kosovo has its own HDI\n",
    "    'Martinique': 'France',\n",
    "    'Mayotte': 'France',\n",
    "    'Monaco': 'France',\n",
    "    'Montserrat': 'United Kingdom',\n",
    "    'Nauru': 'Nauru',\n",
    "    'New Caledonia': 'France',\n",
    "    'Niue': 'New Zealand',\n",
    "    'North Korea': 'North Korea',\n",
    "    'Northern Mariana Islands': 'United States',\n",
    "    'Pitcairn': 'United Kingdom',\n",
    "    'Puerto Rico': 'United States',\n",
    "    'Reunion': 'France',\n",
    "    'Saint Barthelemy': 'France',\n",
    "    'Saint Helena': 'United Kingdom',\n",
    "    'Saint Martin (French part)': 'France',\n",
    "    'Saint Pierre and Miquelon': 'France',\n",
    "    'Sint Maarten (Dutch part)': 'Netherlands',\n",
    "    'Somalia': 'Somalia',\n",
    "    'Tokelau': 'New Zealand',\n",
    "    'Turks and Caicos Islands': 'United Kingdom',\n",
    "    'United States Virgin Islands': 'United States',\n",
    "    'Vatican': 'Italy',\n",
    "    'Wallis and Futuna': 'France'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the territory to its sovereign country:\n",
    "df_covid_cleaned['hdi_source_country'] = df_covid_cleaned['location'].map(territory_to_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup for HDI values of sovereign countries\n",
    "hdi_lookup = df_covid_cleaned.set_index('location')['human_development_index'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing HDI values with the sovereign country’s HDI\n",
    "df_covid_cleaned['human_development_index'] = df_covid_cleaned.apply(\n",
    "    lambda row: hdi_lookup.get(row['hdi_source_country'], row['human_development_index']) \n",
    "    if pd.isna(row['human_development_index']) else row['human_development_index'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_covid_cleaned.drop(columns=['hdi_source_country'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_covid_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which locations have missing HDI values\n",
    "missing_hdi_locations = df_covid_cleaned[df_covid_cleaned['human_development_index'].isna()]\n",
    "print(missing_hdi_locations['location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found data for the human_development_index for 2021 for Nauru and Somalia on the https://hdr.undp.org/ website. We will use this data to fill in the missing values for these two countries in our dataset. We weren't able to find data on the human_development_index for North Korea, so we will impute it with the median value of the HDI for the other countries in the dataset. \n",
    "\n",
    "We will also impute the missing values for the columns total_cases, total_deaths, total_cases_per_million, total_deaths_per_million and life_expectancy with the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing HDI values for Nauru and Somalia and impute North Korea with median value\n",
    "replace_cell(df_covid_cleaned, df_covid_cleaned['location'] == 'Nauru', 'human_development_index', 0.692)\n",
    "replace_cell(df_covid_cleaned, df_covid_cleaned['location'] == 'Somalia', 'human_development_index', 0.385)\n",
    "fill_na_with_median(df_covid_cleaned, \"human_development_index\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_cases\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_deaths\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_cases_per_million\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_deaths_per_million\")\n",
    "fill_na_with_median(df_covid_cleaned, \"life_expectancy\")\n",
    "df_covid_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_covid_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "df_covid_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hypotese 1: Higher population size is associated with higher total COVID-19 deaths, but not necessarily with higher deaths per capita. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 4.1 Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 4.2 Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hypotese 2: Countries with a higher Human Development Index (HDI) have experienced lower COVID-19 death rates per capita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5.1 Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5.2 Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Hypotese 3: Countries with a higher life expectancy and older populations (e.g. higher median age, % aged 65+, etc.) have experienced higher COVID-19 death rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 6.1 Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable is `total_deaths_per_million`, which represents the total number of COVID-19 deaths per million people in each country. This variable is crucial for understanding the impact of the pandemic on different populations and will be used to assess the relationship with independent variables such as `median_age`, `aged_65_older`, `aged_70_older` and `life_expectancy`. \n",
    "\n",
    "It's important to mention, not all countries are represented in the dataset, since the countries with missing data on these variables were removed. This means that the analysis will only include countries for which we have complete data on these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1.1 Descriptive Statistics***\n",
    "\n",
    "First we look at some of the descriptive statistics of the `df_age_cleaned` DataFrame to get an overview of the data. This includes the mean, median, standard deviation, and other statistics for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives summary statistics for all numerical columns in the dataset\n",
    "df_age_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows large variation across countries in both age-related factors and COVID-19 death rates. Median age ranges from 15 to 48 years, and life expectancy from 53 to nearly 85, indicating diverse population structures. COVID-19 deaths per million also vary widely, from 0 to over 6,600, with a high standard deviation—suggesting age and life expectancy could meaningfully relate to differences in death rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1.2 Normality***\n",
    "\n",
    "To tests whether numeric columns follow a normal distribution, we can use the D'Agostino and Jarque-Bera tests. These tests are designed to assess the skewness and kurtosis of the data, which are key indicators of normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test normality of numeric columns\n",
    "def check_normality(df):\n",
    "    num_cols = [col for col in df.select_dtypes(include=['float64', 'int64']).columns if col not in ['location', 'continent']]\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for col in num_cols:\n",
    "        data = df[col]\n",
    "        skewness = data.skew()\n",
    "        kurtosis = data.kurt()\n",
    "        dagostino = stats.normaltest(data)\n",
    "        jb = stats.jarque_bera(data)\n",
    "\n",
    "        normal = \"No\"\n",
    "        if dagostino.pvalue > 0.05 and jb.pvalue > 0.05 and abs(skewness) < 1:\n",
    "            normal = \"Yes\"\n",
    "        elif dagostino.pvalue > 0.01 and abs(skewness) < 2:\n",
    "            normal = \"Partial\"\n",
    "\n",
    "        rows.append({\n",
    "            'Column': col,\n",
    "            'Skewness': round(skewness, 3),\n",
    "            'Kurtosis': round(kurtosis, 3),\n",
    "            \"D'Agostino p-value\": f\"{dagostino.pvalue:.2e}\",\n",
    "            \"Jarque-Bera p-value\": f\"{jb.pvalue:.2e}\",\n",
    "            'Normally Distributed?': normal\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run normality checks on all numeric columns\n",
    "check_normality(df_age_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normality tests show that none of the key variables follow a normal distribution. All columns—total_deaths_per_million, median_age, aged_65_older, aged_70_older, and life_expectancy—exhibit significant skewness and/or kurtosis, with very low p-values from both the D'Agostino and Jarque-Bera tests, confirming deviations from normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purposes, we want to see how the data looks like in histograms. This will help us understand the distribution of the data and identify any potential outliers or skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_selected_histograms(df):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of selected numeric columns from df_age_cleaned with histograms.\n",
    "    \"\"\"\n",
    "    selected_cols = [\n",
    "        'total_deaths_per_million',\n",
    "        'life_expectancy',\n",
    "        'median_age',\n",
    "        'aged_65_older',\n",
    "        'aged_70_older'\n",
    "    ]\n",
    "\n",
    "    n = len(selected_cols)\n",
    "    n_cols = 3\n",
    "    n_rows = (n + n_cols - 1) // n_cols \n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(selected_cols):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "\n",
    "    # Hide unused axes if any\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_selected_histograms(df_age_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the statistical tests and visualizations, none of the numeric variables appear to be normally distributed. The distributions show that total deaths per million and the age-related variables (especially % aged 65+ and 70+) are right-skewed, meaning most countries have lower values but a few have very high ones. In contrast, life expectancy is more normally distributed, and median age is fairly spread out across countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1.3 Outliers***\n",
    "\n",
    "To identify outliers in the `df_age_cleaned` DataFrame, we can use the Interquartile Range (IQR) method. This involves calculating the first (Q1) and third quartiles (Q3) for each numeric column, then determining the IQR as Q3 - Q1. Outliers are defined as values that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through selected columns\n",
    "for column in ['life_expectancy', 'median_age', 'aged_65_older', 'aged_70_older']:\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df_age_cleaned[column].quantile(0.25)\n",
    "    Q3 = df_age_cleaned[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "\n",
    "    # Define the lower and upper bounds for detecting outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Find rows where the value is outside the normal range\n",
    "    outliers = df_age_cleaned[\n",
    "        (df_age_cleaned[column] < lower_bound) | \n",
    "        (df_age_cleaned[column] > upper_bound)\n",
    "    ]\n",
    "\n",
    "    # Print the number of outliers found for the column\n",
    "    print(f\"  {column}: {len(outliers)} outliers detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers in most variables, except for aged_70_older, which has 1 outlier. This indicates that the data is generally consistent, with only one unusually high or low value in the aged_70_older group. We will keep this outlier, as our dataset is small and it may represent a country with unique characteristics that could be important for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore how age-related factors impact COVID-19 deaths, we group `life_expectancy`, `aged 65+`, and `aged 70+` into “low” and “high” categories in order to compare death rates across these groups. We define \"low\" and \"high\" based on the median values of each variable, which allows us to categorize countries into two groups for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_by_age_factors(df):\n",
    "    \"\"\"\n",
    "    Categorizes life_expectancy, aged_65_older, and aged_70_older into\n",
    "    'Low' and 'High' groups and plots boxplots of total_deaths_per_million for each.\n",
    "    \"\"\"\n",
    "    # Define the factors to categorize\n",
    "    factors = ['life_expectancy', 'aged_65_older', 'aged_70_older']\n",
    "    \n",
    "    # Categorize into 'Low' and 'High' using median split\n",
    "    for col in factors:\n",
    "        median_val = df[col].median()\n",
    "        df[f'{col}_group'] = df[col].apply(lambda x: 'Low' if x < median_val else 'High')\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for i, col in enumerate(factors):\n",
    "        sns.boxplot(\n",
    "            x=f'{col}_group',\n",
    "            y='total_deaths_per_million',\n",
    "            data=df,\n",
    "            ax=axes[i],\n",
    "            palette='Set2'\n",
    "        )\n",
    "        axes[i].set_title(f'Deaths per Million by {col.replace(\"_\", \" \").title()} Grouped')\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('Deaths per Million')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_by_age_factors(df_age_cleaned)      \n",
    "\n",
    "# Drop the age group columns after visualization\n",
    "df_age_cleaned = df_age_cleaned.drop(columns=['life_expectancy_group', 'aged_65_older_group', 'aged_70_older_group']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some countries with younger populations or lower life expectancy still experienced high death rates. These outliers suggest that other factors that are not represented in the data (e.g., healthcare quality, policy respons, etc) may also play a significant role.\n",
    "\n",
    "But overall the boxplots show a clear relationship between the age-related variables and total deaths per million. Countries with larger percentages of older populations (aged 65+ and 70+) and longer life expectancies tend to have higher total deaths per million. This suggests that age and life expectancy are important factors in understanding COVID-19 death rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1.4 Correlation***\n",
    "\n",
    "To assess the relationship between age-related factors and COVID-19 death rates, we will use a Heatmap to visualize the correlation matrix of the numeric variables in the `df_age_cleaned` DataFrame. This will help us identify any strong correlations between the variables, particularly between age-related factors and total deaths per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates a correlation matrix from our DataFrame. \n",
    "def my_corr(df):\n",
    "    cormat = df.drop(columns=['continent', 'location']).corr() #checks how strongly each pair of columns are related and drops column 'wine-type'. \n",
    "    return cormat\n",
    "\n",
    "# function that takes the correlation matrix and draws a heatmap (using Seaborn)\n",
    "def my_corr_plot(cormat):\n",
    "    sns.heatmap(cormat, cmap = 'viridis',  annot=True, fmt=\".2f\", square=True, linewidths=.2) #cmap - sets the color style. annot=true - means the numbers will be shown on the heatmap. \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "my_corr_plot(my_corr(df_age_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HeatMap shows a moderate to strong positive correlation between age-related factors and COVID-19 death rates. Median age, percentage aged 65+, and aged 70+ all correlate strongly (~0.66–0.67) with deaths per million, while life expectancy has a moderate correlation (0.52). Median age and life expectancy themselves are highly correlated (0.83), reflecting that countries with older populations tend to have longer life expectancy.\n",
    "\n",
    "The strong correlation between aged 65+ and aged 70+ is expected since these groups overlap (70+ a subset of 65+), indicating multicollinearity that should be considered in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1.5 Scatter Plots***\n",
    "\n",
    "We will create scatter plots to visualize the relationships between total deaths per million and the age-related factors: median age, aged 65+ and life expectancy. This will help us understand how these variables relate to COVID-19 death rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the features and the response using scatterplots\n",
    "sns.pairplot(df_age_cleaned, x_vars=['life_expectancy', 'median_age', 'aged_65_older'], y_vars='total_deaths_per_million', height=5, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots show a positive relationship between total deaths per million and the age-related factors. Especially countries with higher median ages and larger percentages of people aged 65+ have a clear postive relationship and tend to have higher total deaths per million. Life expectancy also shows a positive relationship, but with more variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 6.2 Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train a model to predict total deaths per million based on the age-related factors. We will use a linear regression model for this purpose, as it is a simple yet effective way to understand the relationship between the dependent variable (total deaths per million) and independent variables (median age, aged 65+, aged 70+, and life expectancy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.2.1 Multiple Linear Regression***\n",
    "\n",
    "To assess the relationship between total deaths per million and the age-related factors, we will use a multiple linear regression model. This model will allow us to quantify how each independent variable (median age, aged 65+ and life expectancy) contributes to the dependent variable (total deaths per million)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python list of feature names\n",
    "feature_cols = ['life_expectancy', 'median_age', 'aged_65_older']\n",
    "\n",
    "# Use the list to select a subset of the original DataFrame\n",
    "X = df_age_cleaned[feature_cols]\n",
    "\n",
    "# Print the first 5 rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a Series from the DataFrame for y\n",
    "y = df_age_cleaned['total_deaths_per_million']\n",
    "\n",
    "# Print the first 5 values\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type and shape of X\n",
    "print(f\"Type of X: {type(X)}\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "# Check the type and shape of y\n",
    "print(f\"Type of y: {type(y)}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.1.1 - Now we are going to split X and y variables into training and testing sets. This is important to evaluate the model's performance on unseen data and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Default split 75:25\n",
    "print(f\"Shape of X train: {X_train.shape} and y train: {y_train.shape}\")\n",
    "print(f\"Shape of X test: {X_test.shape} and y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 values\n",
    "print(f\"First 5 rows of X train:\\n{X_test.head()}\")\n",
    "print(f\"First 5 values of y train:\\n{y_test.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.1.2 - We can now create a ultiple linear regression model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Fit the model to our training data\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intercept and coefficients of the model\n",
    "print('b0 =', linreg.intercept_)\n",
    "print('bi =', linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair the feature names with the coefficients\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking and the slopes (coefficients) for life expectancy, median age and aged 65+, we see that: \n",
    "\n",
    "- For each additional year of life expectancy, predicted deaths per million decrease by ~10.6.\n",
    "- For each additional year of median age, predicted deaths per million increase by ~76.3\n",
    "- For each 1% increase in population aged 65+, deaths per million increase by ~54.2.\n",
    "\n",
    "Which indicates that older populations correlate with higher COVID-19 death rates, while higher life expectancy may slightly reduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.1.3 - Now we test the the model with the test data to evaluate its performance. We will use the R-squared value to assess how well the model explains the variance in total deaths per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_predicted = linreg.predict(X_test)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n",
    "print(f\"Mean Absolute Error (MAE): {metrics.mean_absolute_error(y_test, y_predicted)}\")\n",
    "\n",
    "# Mean Squared Error (MSE) is the mean of the squared errors\n",
    "print(f\"Mean Squared Error (MSE): {metrics.mean_squared_error(y_test, y_predicted)}\")\n",
    "\n",
    "# Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors\n",
    "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(metrics.mean_squared_error(y_test, y_predicted))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared\n",
    "print(f\"Explained variance score: {r2_score(y_test, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the regression results\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.scatter(y_test, y_predicted, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model captures a meaningful relationship between age-related factors and COVID-19 death rates, explaining over half the variance in mortality across countries. The positive coefficients for median age and aged 65+ confirm that older populations generally experience higher death rates, while higher life expectancy seems protective or associated with lower deaths.\n",
    "\n",
    "However, the prediction errors (MAE and RMSE) indicate moderate inaccuracies, so other factors beyond age-related variables likely influence COVID-19 deaths as well. This suggests the model is useful for understanding broad trends but may have limitations in precise predictions due to unaccounted variables or data variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.2.2 Decision Tree***\n",
    "\n",
    "To further explore the relationship between age-related factors and COVID-19 death rates, we will use a Decision Tree model. This model will allow us to capture non-linear relationships and interactions between the independent variables and the dependent variable.\n",
    "\n",
    "But first we got to create categories `total_deaths_per_million` in order to use classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the continuous variable into 3 categories using quantiles\n",
    "df_age_cleaned['death_rate_category'] = pd.qcut(df_age_cleaned['total_deaths_per_million'], q=3, labels=['Low', 'Medium', 'High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature columns\n",
    "feature_cols = ['life_expectancy', 'median_age', 'aged_65_older']\n",
    "\n",
    "# Select only those columns plus the target column (total_deaths_per_million)\n",
    "selected_cols = feature_cols + ['death_rate_category']\n",
    "\n",
    "# Extract the subset DataFrame\n",
    "df_subset = df_age_cleaned[selected_cols]\n",
    "\n",
    "# Convert to numpy array\n",
    "array = df_subset.to_numpy()\n",
    "\n",
    "# Create two (sub) arrays from it: \n",
    "# X - features, all rows, all columns but the last one and y - labels, all rows, the last column\n",
    "X, y = array[:, :-1], array[:, -1]\n",
    "\n",
    "# Separate input data into classes based on labels of diagnoses\n",
    "class0 = np.array(X[y==0])\n",
    "class1 = np.array(X[y==1])\n",
    "class2 = np.array(X[y==2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.2.1 - We will split the data into training and testing sets, just like we did for the multiple linear regression model. This is important to evaluate the model's performance on unseen data and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into into training and testing sets in proportion 8:2 - 80% of it as training data and 20% as a validation dataset\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "# Build Decision Trees Classifier \n",
    "classifier = DecisionTreeClassifier(max_depth = 5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the graphviz package for DT visualisation\n",
    "%pip install graphviz\n",
    "import graphviz\n",
    "\n",
    "# Draw tree from the trained data by graphviz package\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None, \n",
    "                         feature_names=feature_cols, class_names = True,        \n",
    "                         filled=True, rounded=True, proportion = False,\n",
    "                         special_characters=True) \n",
    "\n",
    "# Result DT saved in file age.pdf\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"Data/age\") \n",
    "\n",
    "# Show the graph\n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Countries or continents with higher vaccination rates experienced lower COVID-19 death rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 7.1 Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 7.2 Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
